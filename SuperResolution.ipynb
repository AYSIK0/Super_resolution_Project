{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Ayoub Sikouky\n",
    "# Title: SISR Using a Multi Model System\n",
    "# Edit Date: 22/04/2022\n",
    "################################################\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 : Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Data ###########################\n",
    "def download_data(f_name, dst, source_url):\n",
    "    \"\"\"\n",
    "    Download and unzip datastes.\n",
    "    #### Parameters:\n",
    "    f_name: zip file name.\n",
    "    dst: The destination directory where the files will be stored.\n",
    "    source_url: The URL for the dataset.\n",
    "    \"\"\"\n",
    "    file_url = os.path.join(source_url, f_name)\n",
    "    dst_abspath = os.path.abspath(dst)\n",
    "    tf.keras.utils.get_file(f_name, file_url, cache_subdir=dst_abspath, extract=True)\n",
    "    # Removing the zip file.\n",
    "    os.remove(os.path.join(dst_abspath, f_name)) \n",
    "\n",
    "\n",
    "def get_paths(dirs: List[str], path_type):\n",
    "    \"\"\"\n",
    "    Retuns a list of images' paths inside a directories \n",
    "    that have \"path_type\" in their name.\n",
    "    #### Parameters:\n",
    "    dirs: The list of directories to be searched.\n",
    "    path_type: The \"characters\" to selcet which directories to search for images.\n",
    "    \"\"\"\n",
    "    if not isinstance(dirs, list):\n",
    "      raise Exception(\"'dirs' argument must be a list.\")\n",
    "\n",
    "    paths_list = [] # List of paths in \"dirs\"\n",
    "    subpaths_lists = [] # List of lists of images' paths.\n",
    "    for d in dirs:\n",
    "        if path_type in str(d):\n",
    "            paths_list.append(d)\n",
    "    paths_list.sort()\n",
    "    \n",
    "    if len(paths_list) == 1:\n",
    "        img_paths = [str(f) for f in paths_list[0].glob(\"**/*.png\")]\n",
    "        img_paths.sort()\n",
    "        return img_paths\n",
    "    else:\n",
    "      for main_path in paths_list:\n",
    "        tmp_subpath_list = [str(f) for f in main_path.glob(\"**/*.png\") if f.is_file]\n",
    "        tmp_subpath_list.sort()\n",
    "        subpaths_lists.append(tmp_subpath_list)\n",
    "\n",
    "    return subpaths_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the datasets used in training and validation.\n",
    "if not os.path.exists(\"Original_Datasets\"):\n",
    "\n",
    "    DIV2K_URL = \"http://data.vision.ee.ethz.ch/cvl/DIV2K/\"\n",
    "    DIV2K_FILE_NAMES = [\"DIV2K_train_HR.zip\", \"DIV2K_valid_HR.zip\", \n",
    "                        \"DIV2K_train_LR_bicubic_X2.zip\", \"DIV2K_valid_LR_bicubic_X2.zip\"]\n",
    "    dst_dir = \"Original_Datasets/DIV2K\"\n",
    "\n",
    "    for file in DIV2K_FILE_NAMES:\n",
    "        download_data(file, dst_dir, DIV2K_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if colab is used.\n",
    "using_google_colab = \"google.colab\" in str(get_ipython())\n",
    "\n",
    "## When Using Google Colab\n",
    "if using_google_colab:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  colab_path = \"/content/drive/MyDrive/ARU's Master/Application Of Machine Learning/Assignments/Super_Resolution/\"\n",
    "  ALL_DATA_PATH = Path(colab_path+\"Original_Datasets\")\n",
    "  MODELS_PATH = colab_path + \"Models/\"\n",
    "\n",
    "else:\n",
    "## When Using Local Machine.\n",
    "  ALL_DATA_PATH = Path(r\"Original_Datasets\")\n",
    "  MODELS_PATH = \"Models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR Train Len: 800\n",
      "HR Valid Len: 100\n",
      "HR Train Paths Example:\n",
      "['Original_Datasets/DIV2K/DIV2K_train_HR/0001.png', 'Original_Datasets/DIV2K/DIV2K_train_HR/0002.png']\n",
      "LR Train Len: 800\n",
      "LR Valid Len: 100\n",
      "LR Train Paths Example:\n",
      "['Original_Datasets/DIV2K/DIV2K_train_LR_bicubic/X2/0001x2.png', 'Original_Datasets/DIV2K/DIV2K_train_LR_bicubic/X2/0002x2.png']\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "SCALE = 2 # The scale of the downsample images used.\n",
    "\n",
    "# Getting all sub directories inside the working directory.\n",
    "all_paths = ALL_DATA_PATH.glob(\"**/*\")\n",
    "dirs = [d for d in all_paths if d.is_dir()]\n",
    "\n",
    "# Selecting HR (High Resolution) directories.\n",
    "all_sub_paths = get_paths(dirs, \"HR\")\n",
    "\n",
    "# \n",
    "HR_TRAIN_PATH, HR_VALID_PATH = all_sub_paths[0], all_sub_paths[1]\n",
    "print(f\"HR Train Len: {len(HR_TRAIN_PATH)}\")\n",
    "print(f\"HR Valid Len: {len(HR_VALID_PATH)}\")\n",
    "print(f\"HR Train Paths Example:\\n{HR_TRAIN_PATH[:2]}\")\n",
    "\n",
    "LR_TRAIN_PATH, LR_VALID_PATH = get_paths(dirs, f\"X{SCALE}\")\n",
    "print(f\"LR Train Len: {len(LR_TRAIN_PATH)}\")\n",
    "print(f\"LR Valid Len: {len(LR_VALID_PATH)}\")\n",
    "print(f\"LR Train Paths Example:\\n{LR_TRAIN_PATH[:2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creating the datasets used with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b09a82c696b863a0dfbfcea318dd9ac9315936697a1c3f9d0b4de081771d98f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ML_mac')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
